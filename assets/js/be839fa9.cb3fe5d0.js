"use strict";(self.webpackChunkdendrite_docs=self.webpackChunkdendrite_docs||[]).push([[912],{7240:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"conceptual/architecture/data-layer","title":"Data Layer","description":"Receives LSL streams, synchronizes timestamps, and saves raw data to HDF5. Separate queues handle storage and real-time distribution.","source":"@site/docs/conceptual/architecture/data-layer.md","sourceDirName":"conceptual/architecture","slug":"/conceptual/architecture/data-layer","permalink":"/dendrite/docs/conceptual/architecture/data-layer","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"data-layer","title":"Data Layer","sidebar_label":"Data Layer"},"sidebar":"tutorialSidebar","previous":{"title":"Task Application Layer","permalink":"/dendrite/docs/conceptual/architecture/task-application-layer"},"next":{"title":"Processing Layer","permalink":"/dendrite/docs/conceptual/architecture/processing-layer"}}');var a=n(4848),r=n(8453);const i={id:"data-layer",title:"Data Layer",sidebar_label:"Data Layer"},d="Data Layer",c={},l=[{value:"Data Acquisition",id:"data-acquisition",level:2},{value:"Architecture",id:"architecture",level:3},{value:"Markers channel",id:"markers-channel",level:3},{value:"Timestamp Types and Semantics",id:"timestamp-types-and-semantics",level:3},{value:"Latency &amp; Stream Health",id:"latency--stream-health",level:3},{value:"Data Contracts",id:"data-contracts",level:3},{value:"DataSaver",id:"datasaver",level:2},{value:"MetricsSaver",id:"metricssaver",level:2},{value:"Database",id:"database",level:2},{value:"Data Dimension Standard",id:"data-dimension-standard",level:2}];function o(e){const t={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"data-layer",children:"Data Layer"})}),"\n",(0,a.jsx)(t.p,{children:"Receives LSL streams, synchronizes timestamps, and saves raw data to HDF5. Separate queues handle storage and real-time distribution."}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.h2,{id:"data-acquisition",children:"Data Acquisition"}),"\n",(0,a.jsxs)(t.p,{children:["DataAcquisition (",(0,a.jsx)(t.code,{children:"src/dendrite/data/acquisition.py"}),") connects to configured LSL streams (discovered during preflight), uses LSL clock synchronization for timestamp coherence, and distributes data to downstream components."]}),"\n",(0,a.jsx)(t.h3,{id:"architecture",children:"Architecture"}),"\n",(0,a.jsxs)(t.p,{children:["Dedicated reader threads for each stream type pull samples and send to the data queue at native sampling rates. LSL inlets use ",(0,a.jsx)(t.code,{children:"proc_clocksync"})," and ",(0,a.jsx)(t.code,{children:"proc_dejitter"})," flags to correct clock offsets and smooth timestamp jitter."]}),"\n",(0,a.jsx)(t.p,{children:"Each reader builds per-modality payloads from its channel mapping (e.g., an EEG stream with channels typed as 'eeg', 'eog', and 'markers'). Payloads include LSL timestamps and enqueue timestamps for latency tracking."}),"\n",(0,a.jsx)(t.p,{children:"An EEG stream is required for acquisition to start."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Stream Types:"})}),"\n",(0,a.jsx)(t.p,{children:"LSL streams accepted by DataAcquisition, with each stream containing one or more channels classified by modality:"}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"LSL Stream Type"}),(0,a.jsx)(t.th,{children:"Format"}),(0,a.jsx)(t.th,{children:"Required"}),(0,a.jsx)(t.th,{children:"Purpose"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"EEG"})}),(0,a.jsx)(t.td,{children:"Numeric"}),(0,a.jsx)(t.td,{children:"Yes"}),(0,a.jsx)(t.td,{children:"Neural signal acquisition (GUI requirement)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"EMG"})}),(0,a.jsx)(t.td,{children:"Numeric"}),(0,a.jsx)(t.td,{children:"No"}),(0,a.jsx)(t.td,{children:"Muscle signal acquisition"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"EOG"})}),(0,a.jsx)(t.td,{children:"Numeric"}),(0,a.jsx)(t.td,{children:"No"}),(0,a.jsx)(t.td,{children:"Eye movement tracking"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"ContinuousEvents"})}),(0,a.jsx)(t.td,{children:"Numeric"}),(0,a.jsx)(t.td,{children:"No"}),(0,a.jsx)(t.td,{children:"Continuous position/torque data"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"Events"})}),(0,a.jsx)(t.td,{children:"String (JSON)"}),(0,a.jsx)(t.td,{children:"No"}),(0,a.jsxs)(t.td,{children:["Discrete task events (see ",(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"/dendrite/docs/api/generated/dendrite/data/event_outlet",children:"Events API"})}),")"]})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"..."})}),(0,a.jsx)(t.td,{children:"Numeric/String"}),(0,a.jsx)(t.td,{children:"No"}),(0,a.jsx)(t.td,{children:"User-defined types (numerical: forwarded; string: saved only)"})]})]})]}),"\n",(0,a.jsxs)(t.p,{children:["Stream types are user-definable LSL type names from the stream's ",(0,a.jsx)(t.code,{children:"type()"})," field. Only ",(0,a.jsx)(t.strong,{children:"Events"})," streams have special behavior (JSON parsing; event IDs are injected into Markers by the processor). All numerical streams are forwarded to processing at their native rates. String-formatted streams are saved to disk only."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Channel Types:"})}),"\n",(0,a.jsx)(t.p,{children:"Channel types are user-defined labels classifying individual channels by modality. A single LSL stream can contain mixed channel types (e.g., an EEG stream with 62 EEG channels, 2 EOG channels, and 1 Markers channel). Common types: EEG, EMG, EOG, Markers, Reference."}),"\n",(0,a.jsx)(t.h3,{id:"markers-channel",children:"Markers channel"}),"\n",(0,a.jsx)(t.p,{children:"The Markers channel embeds event trigger values into data samples, enabling event-locked epoch extraction."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Event injection"})," is the canonical mechanism: when events arrive from the Events stream, the Data Processor queues them and injects the ",(0,a.jsx)(t.code,{children:"event_id"})," into the ",(0,a.jsx)(t.code,{children:"markers"})," field of subsequent data samples. Modes detect events by reading the markers channel (non-zero values indicate event triggers). The processor ensures each configured data stream receives the marker before removing it from the queue."]}),"\n",(0,a.jsx)(t.p,{children:"When a stream has EEG channels but no hardware markers, a synthetic Markers channel is added during stream setup (initialized to zero)."}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Deprecated:"})," Physical marker extraction from hardware trigger channels (",(0,a.jsx)(t.code,{children:"marker_index"}),") is legacy functionality for backward compatibility with older recordings. This will be removed in a future release. Use the Events stream for all new implementations."]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"The storage layer preserves events separately as discrete records with precise timestamps for offline analysis, in addition to the marker values embedded in data samples."}),"\n",(0,a.jsxs)(t.p,{children:["See ",(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"/dendrite/docs/conceptual/architecture/processing-layer#synchronousmode",children:"Synchronous Mode"})})," for epoch triggering and ",(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"/dendrite/docs/conceptual/architecture/task-application-layer#sending-events",children:"Sending Events"})})," for event broadcasting."]}),"\n",(0,a.jsx)(t.h3,{id:"timestamp-types-and-semantics",children:"Timestamp Types and Semantics"}),"\n",(0,a.jsx)(t.p,{children:"The system captures multiple timestamps at different pipeline stages for timing analysis and latency tracking."}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Timestamp"}),(0,a.jsx)(t.th,{children:"Source"}),(0,a.jsx)(t.th,{children:"Layer"}),(0,a.jsx)(t.th,{children:"Purpose"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsxs)(t.td,{children:[(0,a.jsx)(t.code,{children:"timestamp"})," / ",(0,a.jsx)(t.code,{children:"lsl_timestamp"})]}),(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"inlet.pull_sample()"})}),(0,a.jsx)(t.td,{children:"Data/Processing"}),(0,a.jsx)(t.td,{children:"LSL-synchronized capture time"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"local_timestamp"})}),(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"local_clock()"})}),(0,a.jsx)(t.td,{children:"Storage"}),(0,a.jsx)(t.td,{children:"Local receive time for HDF5"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"_daq_receive_ns"})}),(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"time.time_ns()"})}),(0,a.jsx)(t.td,{children:"Internal"}),(0,a.jsx)(t.td,{children:"Pipeline latency tracking"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"data_timestamp"})}),(0,a.jsxs)(t.td,{children:["Copied from ",(0,a.jsx)(t.code,{children:"lsl_timestamp"})]}),(0,a.jsx)(t.td,{children:"Mode Output"}),(0,a.jsx)(t.td,{children:"E2E latency for task apps"})]})]})]}),"\n",(0,a.jsx)(t.h3,{id:"latency--stream-health",children:"Latency & Stream Health"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"LSL transmission latency"})," (",(0,a.jsx)(t.code,{children:"local_timestamp - timestamp"}),") measures transmission delay from device capture to DAQ receive (2-10ms typical). All streams warn if latency exceeds 50ms. Latency does not accumulate; each sample has independent delay and bounded queues prevent backlog buildup. Per-stream metrics (rolling P50 latency, last update timestamp) are published to SharedState for telemetry display."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Stream drops"}),': Each stream thread operates independently. Dropped streams hold last value while others continue. TelemetryWidget shows "DROPPED" after timeout. Saved files show drops as timestamp gaps; original timestamps preserved for post-hoc alignment. Streams reconnect automatically when source resumes.']}),"\n",(0,a.jsx)(t.h3,{id:"data-contracts",children:"Data Contracts"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Input (stream configuration):"})}),"\n",(0,a.jsxs)(t.p,{children:["Stream configurations use ",(0,a.jsx)(t.code,{children:"StreamMetadata"})," Pydantic schemas (",(0,a.jsx)(t.code,{children:"src/dendrite/data/stream_schemas.py"}),") containing stream identification (name, type, uid), channel information (count, labels, types, units), timing (sample rate), and validation tracking. Events streams use string ",(0,a.jsx)(t.code,{children:"channel_format"})," instead of numeric."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Output to Processing Layer (via data_queue):"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"{\n  'data': {\n    '<modality>': np.ndarray,        # Shape: (n_channels, 1) - per channel_type from stream\n    'markers': np.ndarray,           # Shape: (1, 1) - synthetic or physical markers\n  },  # Keys are dynamic based on channel_types in stream config (e.g., 'eeg', 'eog', 'emg')\n  'stream_name': str,                # Stream identifier for routing\n  'lsl_timestamp': float,            # LSL synchronized timestamp\n  '_daq_receive_ns': int,            # DAQ receive time (latency tracking)\n  '{stream_type}_latency_ms': float, # Dynamic key (e.g., eeg_latency_ms, emg_latency_ms)\n}\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"To Storage (via save_queue):"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"@dataclass\nclass DataRecord:\n    \"\"\"Data structure for holding samples and metadata.\"\"\"\n    modality: str          # Stream type: 'EEG', 'EMG', 'Event', 'String'\n    sample: Any            # Timeseries: np.ndarray; Events: [json_string]; Strings: [str]\n    timestamp: float       # LSL synchronized timestamp (from inlet.pull_sample())\n    local_timestamp: float # Local machine timestamp when received (from local_clock())\n"})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Example:"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# Timeseries\nDataRecord(modality='EEG', sample=np.array([0.1, 0.2]), timestamp=1234.567, local_timestamp=1234.568)\n\n# Event\nevent_json = json.dumps({'event_id': 10, 'event_type': 'target_onset'})\nDataRecord(modality='Event', sample=[event_json], timestamp=1234.567, local_timestamp=1234.568)\n"})}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.h2,{id:"datasaver",children:"DataSaver"}),"\n",(0,a.jsxs)(t.p,{children:["DataSaver (",(0,a.jsx)(t.code,{children:"src/dendrite/data/storage/data_saver.py"}),") runs in a separate process, buffering data records and writing chunked HDF5 datasets for real-time isolation."]}),"\n",(0,a.jsx)(t.p,{children:"The saver consumes DataRecord objects from the save queue, routing each to modality-specific handlers. Chunked writes and periodic flushes provide crash resistance. The HDF5 file uses a hierarchical structure for cross-platform compatibility (Python/MATLAB), with each modality as a structured dataset containing one field per channel plus timestamps. Events use a compound datatype storing event_id, event_type, timestamps, and extra_vars (JSON metadata)."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"File Structure:"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:"<session>.h5\n\u251c\u2500\u2500 EEG                 # Structured dataset with channel fields\n\u251c\u2500\u2500 EMG                 # Structured dataset (if present)\n\u251c\u2500\u2500 Event               # Structured event dataset\n\u2514\u2500\u2500 <OtherModalities>   # Additional datasets as configured\nattrs: created_timestamp, created_by, version (metadata added via Metadata records)\n"})}),"\n",(0,a.jsx)(t.p,{children:"Data is stored using NumPy structured arrays (compound datatypes) with named fields for each channel plus timestamps. Timeseries datasets (EEG, EMG) use float32 channel data with float64 timestamps. Event datasets store event_id, event_type, timestamps, and extra_vars (JSON metadata)."}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.h2,{id:"metricssaver",children:"MetricsSaver"}),"\n",(0,a.jsxs)(t.p,{children:["MetricsSaver (",(0,a.jsx)(t.code,{children:"src/dendrite/data/storage/metrics_saver.py"}),") persists mode outputs (predictions, confidences, timing metrics) to a separate HDF5 file for offline analysis."]}),"\n",(0,a.jsxs)(t.p,{children:["The saver consumes output records from Processing Modes via ",(0,a.jsx)(t.code,{children:"mode_metrics_queues"})," (dict mapping mode names to their output queues) and creates an HDF5 group for each mode using its configured name. Datasets are created dynamically based on mode output, with common metrics including predictions, confidences, and processing times. Each metric gets a parallel ",(0,a.jsx)(t.code,{children:"{metric}_timestamps"})," dataset for temporal alignment."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"File Structure:"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-text",children:'<session>_metrics.h5\n\u251c\u2500\u2500 <mode_name>/               # e.g., "sync_mode_0", "async_mode_0"\n\u2502   \u251c\u2500\u2500 <metric_key>           # Dynamic from mode output (predictions, confidences, etc.)\n\u2502   \u251c\u2500\u2500 <metric_key>_timestamps # Float64 epoch timestamps for temporal alignment\n\u2502   \u2514\u2500\u2500 ...                    # Additional metrics as emitted by mode\n'})}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Note:"})," The metrics schema is data-driven. Datasets are created dynamically from mode output packets rather than following a fixed structure."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Usage:"})," Metrics file path registered in recordings table, linking mode outputs to source session for experiment lineage tracing."]}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.h2,{id:"database",children:"Database"}),"\n",(0,a.jsxs)(t.p,{children:["Database (",(0,a.jsx)(t.code,{children:"src/dendrite/data/storage/database.py"}),") tracks experiment lineage and recording metadata through an SQLite database located at ",(0,a.jsx)(t.code,{children:"data/dendrite.db"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["The database uses a repository pattern with four tables: ",(0,a.jsx)(t.code,{children:"studies"})," (master organization), ",(0,a.jsx)(t.code,{children:"recordings"})," (session metadata with file paths and BIDS fields), ",(0,a.jsx)(t.code,{children:"datasets"})," (imported FIF files with preprocessing parameters), and ",(0,a.jsx)(t.code,{children:"decoders"})," (trained model metadata including accuracy metrics). All repositories use parameterized queries for SQL injection protection and context managers for cleanup. Tables are indexed on names and foreign keys for fast lineage queries."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Usage:"})," The Main Window registers new sessions via ",(0,a.jsx)(t.code,{children:"add_recording()"}),", the ML Workbench registers trained models via ",(0,a.jsx)(t.code,{children:"add_decoder()"}),", and the DB Explorer provides a GUI for browsing the database."]}),"\n",(0,a.jsxs)(t.p,{children:["See ",(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"/dendrite/docs/conceptual/architecture/auxiliary-layer#database-explorer",children:"Auxiliary Layer"})})," for complete schema documentation."]}),"\n",(0,a.jsx)(t.h2,{id:"data-dimension-standard",children:"Data Dimension Standard"}),"\n",(0,a.jsxs)(t.p,{children:["Dendrite uses a consistent ",(0,a.jsx)(t.strong,{children:"(channels, times)"})," format throughout the entire processing pipeline:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"Data Processor: Extracts channels by type \u2192 (channels, samples) chunks\n                \u2193 Send individual samples as (n_channels, 1)\nMode Buffers:   Accumulate \u2192 [(n_channels, 1), (n_channels, 1), ...]\n                \u2193 Direct concatenation along time axis\nMode Output:    np.concatenate(axis=1) \u2192 (n_channels, n_times)\n                \u2193 Add batch dimension in decoder\nDecoder Input:  (batch, n_channels, n_times)\n                \u2193 Model-specific transforms (STFT, etc.)\nModel Input:    \u2022 Time-series: (batch, 1, n_channels, n_times)\n                \u2022 Time-frequency: (batch, n_channels, n_frequencies, n_times)\n"})}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Related Documentation:"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"/dendrite/docs/conceptual/architecture/task-application-layer",children:"Task Application Layer"})})," - Event creation and broadcasting"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"/dendrite/docs/conceptual/architecture/processing-layer",children:"Processing Layer"})})," - Real-time data processing workflows"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"/dendrite/docs/conceptual/architecture/auxiliary-layer",children:"Auxiliary Layer"})})," - Offline analysis and stream management"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"API References:"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.a,{href:"/dendrite/docs/api/generated/dendrite/data/event_outlet",children:"Events API"})})," - Event stream format and integration"]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(o,{...e})}):o(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>d});var s=n(6540);const a={},r=s.createContext(a);function i(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);